{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1ony-U_gDCL",
        "outputId": "44a21c7f-81c2-4b79-f2ee-46ca8933d9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio: [0.525783182686236, 0.22542532144494884, 0.16650776678532875]\n",
            "+------------------+-------------------+--------------------+--------+--------+--------+--------+\n",
            "|pc_1              |pc_2               |pc_3                |NO2_Mean|O3_Mean |SO2_Mean|CO_Mean |\n",
            "+------------------+-------------------+--------------------+--------+--------+--------+--------+\n",
            "|1.737732749912841 |0.40158725623237845|0.10359616296396407 |4.541667|0.038458|0.958333|0.025   |\n",
            "|1.7451441500302567|0.4032987370179101 |0.09608461043974453 |4.541667|0.038458|0.958333|0.020833|\n",
            "|1.7419062347178151|0.3912312407873393 |0.10764207430090147 |4.541667|0.038458|0.925   |0.025   |\n",
            "|1.749317634835231 |0.3929427215728709 |0.10013052177668194 |4.541667|0.038458|0.925   |0.020833|\n",
            "|0.6286862157919038|0.0745221420956521 |-0.6099443236202006 |10.0    |0.025208|1.875   |0.095833|\n",
            "|0.6286862157919038|0.0745221420956521 |-0.6099443236202006 |10.0    |0.025208|1.875   |0.095833|\n",
            "|0.6286862157919038|0.0745221420956521 |-0.6099443236202006 |10.0    |0.025208|1.875   |0.095833|\n",
            "|0.6286862157919038|0.0745221420956521 |-0.6099443236202006 |10.0    |0.025208|1.875   |0.095833|\n",
            "|0.9764729676238928|1.496027585465394  |0.005548646957150516|8.583333|0.039958|4.375   |0.1     |\n",
            "|0.9690615675064773|1.4943161046798625 |0.013060199481369883|8.583333|0.039958|4.375   |0.104167|\n",
            "+------------------+-------------------+--------------------+--------+--------+--------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "INPUT_PATHS = [\n",
        "    \"/content/part-00000-4f3bd938-470a-46cb-940f-48b68d9cdb1b-c000.csv\",\n",
        "    \"/content/part-00001-4f3bd938-470a-46cb-940f-48b68d9cdb1b-c000.csv\",\n",
        "    \"/content/part-00002-4f3bd938-470a-46cb-940f-48b68d9cdb1b-c000.csv\"\n",
        "]\n",
        "\n",
        "FEATURE_COLUMNS = [\"NO2_Mean\", \"O3_Mean\", \"SO2_Mean\", \"CO_Mean\"]\n",
        "N_COMPONENTS = 3\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SAT5165_PCA_Sucharitha\").getOrCreate()\n",
        "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(INPUT_PATHS)\n",
        "\n",
        "for col in FEATURE_COLUMNS:\n",
        "    df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
        "\n",
        "non_null_condition = None\n",
        "for c in FEATURE_COLUMNS:\n",
        "    cond = F.col(c).isNotNull()\n",
        "    non_null_condition = cond if non_null_condition is None else (non_null_condition | cond)\n",
        "df = df.filter(non_null_condition)\n",
        "\n",
        "medians = {c: df.approxQuantile(c, [0.5], 0.001)[0] for c in FEATURE_COLUMNS}\n",
        "fill_map = {k: (v if v is not None else 0.0) for k, v in medians.items()}\n",
        "df = df.fillna(fill_map)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features_unscaled\")\n",
        "scaler = StandardScaler(inputCol=\"features_unscaled\", outputCol=\"features\", withMean=True, withStd=True)\n",
        "pca = PCA(k=N_COMPONENTS, inputCol=\"features\", outputCol=\"pca_features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
        "model = pipeline.fit(df)\n",
        "transformed = model.transform(df)\n",
        "\n",
        "explained_variance = model.stages[-1].explainedVariance.toArray().tolist()\n",
        "print(\"Explained variance ratio:\", explained_variance)\n",
        "\n",
        "# Extract PCA component values safely using a UDF\n",
        "def extract_pca_value(v, idx):\n",
        "    try:\n",
        "        return float(v.toArray()[idx])\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "extract_udf = F.udf(lambda v, idx: extract_pca_value(v, idx), DoubleType())\n",
        "\n",
        "for i in range(N_COMPONENTS):\n",
        "    transformed = transformed.withColumn(f\"pc_{i+1}\", extract_udf(F.col(\"pca_features\"), F.lit(i)))\n",
        "\n",
        "transformed.select([f\"pc_{i+1}\" for i in range(N_COMPONENTS)] + FEATURE_COLUMNS).show(10, truncate=False)\n",
        "\n",
        "spark.stop()\n"
      ]
    }
  ]
}