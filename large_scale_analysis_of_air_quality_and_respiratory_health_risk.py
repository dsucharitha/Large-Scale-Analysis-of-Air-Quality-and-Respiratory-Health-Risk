# -*- coding: utf-8 -*-
"""Large-Scale-Analysis-of-Air-Quality-and-Respiratory-Health-Risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Awz8sQpVz4bTMgxupFCd6THsuSePn7Qe
"""

from pyspark.sql import SparkSession
from pyspark.sql.types import DoubleType
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA
from pyspark.ml import Pipeline

INPUT_PATHS = [
    "/content/part-00000-4f3bd938-470a-46cb-940f-48b68d9cdb1b-c000.csv",
    "/content/part-00001-4f3bd938-470a-46cb-940f-48b68d9cdb1b-c000.csv",
    "/content/part-00002-4f3bd938-470a-46cb-940f-48b68d9cdb1b-c000.csv"
]

FEATURE_COLUMNS = ["NO2_Mean", "O3_Mean", "SO2_Mean", "CO_Mean"]
N_COMPONENTS = 3

spark = SparkSession.builder.appName("SAT5165_PCA_Sucharitha").getOrCreate()
df = spark.read.option("header", True).option("inferSchema", True).csv(INPUT_PATHS)

for col in FEATURE_COLUMNS:
    df = df.withColumn(col, F.col(col).cast(DoubleType()))

non_null_condition = None
for c in FEATURE_COLUMNS:
    cond = F.col(c).isNotNull()
    non_null_condition = cond if non_null_condition is None else (non_null_condition | cond)
df = df.filter(non_null_condition)

medians = {c: df.approxQuantile(c, [0.5], 0.001)[0] for c in FEATURE_COLUMNS}
fill_map = {k: (v if v is not None else 0.0) for k, v in medians.items()}
df = df.fillna(fill_map)

assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol="features_unscaled")
scaler = StandardScaler(inputCol="features_unscaled", outputCol="features", withMean=True, withStd=True)
pca = PCA(k=N_COMPONENTS, inputCol="features", outputCol="pca_features")

pipeline = Pipeline(stages=[assembler, scaler, pca])
model = pipeline.fit(df)
transformed = model.transform(df)

explained_variance = model.stages[-1].explainedVariance.toArray().tolist()
print("Explained variance ratio:", explained_variance)

# Extract PCA component values safely using a UDF
def extract_pca_value(v, idx):
    try:
        return float(v.toArray()[idx])
    except Exception:
        return None

extract_udf = F.udf(lambda v, idx: extract_pca_value(v, idx), DoubleType())

for i in range(N_COMPONENTS):
    transformed = transformed.withColumn(f"pc_{i+1}", extract_udf(F.col("pca_features"), F.lit(i)))

transformed.select([f"pc_{i+1}" for i in range(N_COMPONENTS)] + FEATURE_COLUMNS).show(10, truncate=False)

spark.stop()